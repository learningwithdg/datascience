{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Features\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_home\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Load a dataset from the online repository (requires internet).\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "name : str\n",
       "    Name of the dataset (`name`.csv on\n",
       "    https://github.com/mwaskom/seaborn-data).  You can obtain list of\n",
       "    available datasets using :func:`get_dataset_names`\n",
       "cache : boolean, optional\n",
       "    If True, then cache data locally and use the cache on subsequent calls\n",
       "data_home : string, optional\n",
       "    The directory in which to cache data. By default, uses ~/seaborn-data/\n",
       "kws : dict, optional\n",
       "    Passed to pandas.read_csv\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/pyml/lib/python3.7/site-packages/seaborn/utils.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.load_dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"price\": 8_500_000, \"rooms\": 3, \"loc\": \"Kathmandu\"},\n",
    "    {\"price\": 7_000_000, \"rooms\": 4, \"loc\": \"Biratnagar\"},\n",
    "    {\"price\": 9_850_000, \"rooms\": 3, \"loc\": \"Pokhara\"},\n",
    "    {\"price\": 7_500_000, \"rooms\": 5, \"loc\": \"Birgunj\"},\n",
    "    {\"price\": 5_000_000, \"rooms\": 3, \"loc\": \"Birgunj\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc</th>\n",
       "      <th>price</th>\n",
       "      <th>rooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>8500000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Biratnagar</td>\n",
       "      <td>7000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pokhara</td>\n",
       "      <td>9850000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Birgunj</td>\n",
       "      <td>7500000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Birgunj</td>\n",
       "      <td>5000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          loc    price  rooms\n",
       "0   Kathmandu  8500000      3\n",
       "1  Biratnagar  7000000      4\n",
       "2     Pokhara  9850000      3\n",
       "3     Birgunj  7500000      5\n",
       "4     Birgunj  5000000      3"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mDictVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'numpy.float64'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseparator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Transforms lists of feature-value mappings to vectors.\n",
       "\n",
       "This transformer turns lists of mappings (dict-like objects) of feature\n",
       "names to feature values into Numpy arrays or scipy.sparse matrices for use\n",
       "with scikit-learn estimators.\n",
       "\n",
       "When feature values are strings, this transformer will do a binary one-hot\n",
       "(aka one-of-K) coding: one boolean-valued feature is constructed for each\n",
       "of the possible string values that the feature can take on. For instance,\n",
       "a feature \"f\" that can take on the values \"ham\" and \"spam\" will become two\n",
       "features in the output, one signifying \"f=ham\", the other \"f=spam\".\n",
       "\n",
       "However, note that this transformer will only do a binary one-hot encoding\n",
       "when feature values are of type string. If categorical features are\n",
       "represented as numeric values such as int, the DictVectorizer can be\n",
       "followed by :class:`sklearn.preprocessing.OneHotEncoder` to complete\n",
       "binary one-hot encoding.\n",
       "\n",
       "Features that do not occur in a sample (mapping) will have a zero value\n",
       "in the resulting array/matrix.\n",
       "\n",
       "Read more in the :ref:`User Guide <dict_feature_extraction>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "dtype : callable, optional\n",
       "    The type of feature values. Passed to Numpy array/scipy.sparse matrix\n",
       "    constructors as the dtype argument.\n",
       "separator : string, optional\n",
       "    Separator string used when constructing new features for one-hot\n",
       "    coding.\n",
       "sparse : boolean, optional.\n",
       "    Whether transform should produce scipy.sparse matrices.\n",
       "    True by default.\n",
       "sort : boolean, optional.\n",
       "    Whether ``feature_names_`` and ``vocabulary_`` should be\n",
       "    sorted when fitting. True by default.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "vocabulary_ : dict\n",
       "    A dictionary mapping feature names to feature indices.\n",
       "\n",
       "feature_names_ : list\n",
       "    A list of length n_features containing the feature names (e.g., \"f=ham\"\n",
       "    and \"f=spam\").\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.feature_extraction import DictVectorizer\n",
       ">>> v = DictVectorizer(sparse=False)\n",
       ">>> D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n",
       ">>> X = v.fit_transform(D)\n",
       ">>> X\n",
       "array([[2., 0., 1.],\n",
       "       [0., 1., 3.]])\n",
       ">>> v.inverse_transform(X) ==         [{'bar': 2.0, 'foo': 1.0}, {'baz': 1.0, 'foo': 3.0}]\n",
       "True\n",
       ">>> v.transform({'foo': 4, 'unseen_feature': 3})\n",
       "array([[0., 0., 4.]])\n",
       "\n",
       "See also\n",
       "--------\n",
       "FeatureHasher : performs vectorization using only a hash function.\n",
       "sklearn.preprocessing.OrdinalEncoder : handles nominal/categorical\n",
       "  features encoded as columns of arbitrary data types.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/pyml/lib/python3.7/site-packages/sklearn/feature_extraction/dict_vectorizer.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DictVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      0,       0,       1,       0, 8500000,       3],\n",
       "       [      1,       0,       0,       0, 7000000,       4],\n",
       "       [      0,       0,       0,       1, 9850000,       3],\n",
       "       [      0,       1,       0,       0, 7500000,       5],\n",
       "       [      0,       1,       0,       0, 5000000,       3]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = DictVectorizer(sparse=False, dtype=int)\n",
    "vec_1 = vec.fit_transform(data)\n",
    "vec_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loc=Biratnagar',\n",
       " 'loc=Birgunj',\n",
       " 'loc=Kathmandu',\n",
       " 'loc=Pokhara',\n",
       " 'price',\n",
       " 'rooms']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loc=Biratnagar',\n",
       " 'loc=Birgunj',\n",
       " 'loc=Kathmandu',\n",
       " 'loc=Pokhara',\n",
       " 'price',\n",
       " 'rooms']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc=Biratnagar</th>\n",
       "      <th>loc=Birgunj</th>\n",
       "      <th>loc=Kathmandu</th>\n",
       "      <th>loc=Pokhara</th>\n",
       "      <th>price</th>\n",
       "      <th>rooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8500000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9850000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7500000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loc=Biratnagar  loc=Birgunj  loc=Kathmandu  loc=Pokhara    price  rooms\n",
       "0               0            0              1            0  8500000      3\n",
       "1               1            0              0            0  7000000      4\n",
       "2               0            0              0            1  9850000      3\n",
       "3               0            1              0            0  7500000      5\n",
       "4               0            1              0            0  5000000      3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(vec_1, columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [\n",
    "    \"this is broken\",\n",
    "    \"does not work after couple of days\",\n",
    "    \"is not what is advertised\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdecode_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpreprocessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'(?u)\\\\b\\\\w\\\\w+\\\\b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'numpy.int64'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Convert a collection of text documents to a matrix of token counts\n",
       "\n",
       "This implementation produces a sparse representation of the counts using\n",
       "scipy.sparse.csr_matrix.\n",
       "\n",
       "If you do not provide an a-priori dictionary and you do not use an analyzer\n",
       "that does some kind of feature selection then the number of features will\n",
       "be equal to the vocabulary size found by analyzing the data.\n",
       "\n",
       "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "input : string {'filename', 'file', 'content'}\n",
       "    If 'filename', the sequence passed as an argument to fit is\n",
       "    expected to be a list of filenames that need reading to fetch\n",
       "    the raw content to analyze.\n",
       "\n",
       "    If 'file', the sequence items must have a 'read' method (file-like\n",
       "    object) that is called to fetch the bytes in memory.\n",
       "\n",
       "    Otherwise the input is expected to be the sequence strings or\n",
       "    bytes items are expected to be analyzed directly.\n",
       "\n",
       "encoding : string, 'utf-8' by default.\n",
       "    If bytes or files are given to analyze, this encoding is used to\n",
       "    decode.\n",
       "\n",
       "decode_error : {'strict', 'ignore', 'replace'}\n",
       "    Instruction on what to do if a byte sequence is given to analyze that\n",
       "    contains characters not of the given `encoding`. By default, it is\n",
       "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
       "    values are 'ignore' and 'replace'.\n",
       "\n",
       "strip_accents : {'ascii', 'unicode', None}\n",
       "    Remove accents and perform other character normalization\n",
       "    during the preprocessing step.\n",
       "    'ascii' is a fast method that only works on characters that have\n",
       "    an direct ASCII mapping.\n",
       "    'unicode' is a slightly slower method that works on any characters.\n",
       "    None (default) does nothing.\n",
       "\n",
       "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
       "    :func:`unicodedata.normalize`.\n",
       "\n",
       "lowercase : boolean, True by default\n",
       "    Convert all characters to lowercase before tokenizing.\n",
       "\n",
       "preprocessor : callable or None (default)\n",
       "    Override the preprocessing (string transformation) stage while\n",
       "    preserving the tokenizing and n-grams generation steps.\n",
       "\n",
       "tokenizer : callable or None (default)\n",
       "    Override the string tokenization step while preserving the\n",
       "    preprocessing and n-grams generation steps.\n",
       "    Only applies if ``analyzer == 'word'``.\n",
       "\n",
       "stop_words : string {'english'}, list, or None (default)\n",
       "    If 'english', a built-in stop word list for English is used.\n",
       "    There are several known issues with 'english' and you should\n",
       "    consider an alternative (see :ref:`stop_words`).\n",
       "\n",
       "    If a list, that list is assumed to contain stop words, all of which\n",
       "    will be removed from the resulting tokens.\n",
       "    Only applies if ``analyzer == 'word'``.\n",
       "\n",
       "    If None, no stop words will be used. max_df can be set to a value\n",
       "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
       "    words based on intra corpus document frequency of terms.\n",
       "\n",
       "token_pattern : string\n",
       "    Regular expression denoting what constitutes a \"token\", only used\n",
       "    if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
       "    or more alphanumeric characters (punctuation is completely ignored\n",
       "    and always treated as a token separator).\n",
       "\n",
       "ngram_range : tuple (min_n, max_n)\n",
       "    The lower and upper boundary of the range of n-values for different\n",
       "    n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
       "    will be used.\n",
       "\n",
       "analyzer : string, {'word', 'char', 'char_wb'} or callable\n",
       "    Whether the feature should be made of word or character n-grams.\n",
       "    Option 'char_wb' creates character n-grams only from text inside\n",
       "    word boundaries; n-grams at the edges of words are padded with space.\n",
       "\n",
       "    If a callable is passed it is used to extract the sequence of features\n",
       "    out of the raw, unprocessed input.\n",
       "\n",
       "    .. versionchanged:: 0.21\n",
       "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
       "    first read from the file and then passed to the given callable\n",
       "    analyzer.\n",
       "\n",
       "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
       "    When building the vocabulary ignore terms that have a document\n",
       "    frequency strictly higher than the given threshold (corpus-specific\n",
       "    stop words).\n",
       "    If float, the parameter represents a proportion of documents, integer\n",
       "    absolute counts.\n",
       "    This parameter is ignored if vocabulary is not None.\n",
       "\n",
       "min_df : float in range [0.0, 1.0] or int, default=1\n",
       "    When building the vocabulary ignore terms that have a document\n",
       "    frequency strictly lower than the given threshold. This value is also\n",
       "    called cut-off in the literature.\n",
       "    If float, the parameter represents a proportion of documents, integer\n",
       "    absolute counts.\n",
       "    This parameter is ignored if vocabulary is not None.\n",
       "\n",
       "max_features : int or None, default=None\n",
       "    If not None, build a vocabulary that only consider the top\n",
       "    max_features ordered by term frequency across the corpus.\n",
       "\n",
       "    This parameter is ignored if vocabulary is not None.\n",
       "\n",
       "vocabulary : Mapping or iterable, optional\n",
       "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
       "    indices in the feature matrix, or an iterable over terms. If not\n",
       "    given, a vocabulary is determined from the input documents. Indices\n",
       "    in the mapping should not be repeated and should not have any gap\n",
       "    between 0 and the largest index.\n",
       "\n",
       "binary : boolean, default=False\n",
       "    If True, all non zero counts are set to 1. This is useful for discrete\n",
       "    probabilistic models that model binary events rather than integer\n",
       "    counts.\n",
       "\n",
       "dtype : type, optional\n",
       "    Type of the matrix returned by fit_transform() or transform().\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "vocabulary_ : dict\n",
       "    A mapping of terms to feature indices.\n",
       "\n",
       "stop_words_ : set\n",
       "    Terms that were ignored because they either:\n",
       "\n",
       "      - occurred in too many documents (`max_df`)\n",
       "      - occurred in too few documents (`min_df`)\n",
       "      - were cut off by feature selection (`max_features`).\n",
       "\n",
       "    This is only available if no vocabulary was given.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
       ">>> corpus = [\n",
       "...     'This is the first document.',\n",
       "...     'This document is the second document.',\n",
       "...     'And this is the third one.',\n",
       "...     'Is this the first document?',\n",
       "... ]\n",
       ">>> vectorizer = CountVectorizer()\n",
       ">>> X = vectorizer.fit_transform(corpus)\n",
       ">>> print(vectorizer.get_feature_names())\n",
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
       ">>> print(X.toarray())  # doctest: +NORMALIZE_WHITESPACE\n",
       "[[0 1 1 1 0 0 1 0 1]\n",
       " [0 2 0 1 0 1 1 0 1]\n",
       " [1 0 0 1 1 0 1 1 1]\n",
       " [0 1 1 1 0 0 1 0 1]]\n",
       "\n",
       "See also\n",
       "--------\n",
       "HashingVectorizer, TfidfVectorizer\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The ``stop_words_`` attribute can get large and increase the model size\n",
       "when pickling. This attribute is provided only for introspection and can\n",
       "be safely removed using delattr or set to None before pickling.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/pyml/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     TfidfVectorizer\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CountVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x12 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 14 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(sample)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>advertised</th>\n",
       "      <th>after</th>\n",
       "      <th>broken</th>\n",
       "      <th>couple</th>\n",
       "      <th>days</th>\n",
       "      <th>does</th>\n",
       "      <th>is</th>\n",
       "      <th>not</th>\n",
       "      <th>of</th>\n",
       "      <th>this</th>\n",
       "      <th>what</th>\n",
       "      <th>work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   advertised  after  broken  couple  days  does  is  not  of  this  what  \\\n",
       "0           0      0       1       0     0     0   1    0   0     1     0   \n",
       "1           0      1       0       1     1     1   0    1   1     0     0   \n",
       "2           1      0       0       0     0     0   2    1   0     0     1   \n",
       "\n",
       "   work  \n",
       "0     0  \n",
       "1     1  \n",
       "2     0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>advertised</th>\n",
       "      <th>after</th>\n",
       "      <th>broken</th>\n",
       "      <th>couple</th>\n",
       "      <th>days</th>\n",
       "      <th>does</th>\n",
       "      <th>is</th>\n",
       "      <th>not</th>\n",
       "      <th>of</th>\n",
       "      <th>this</th>\n",
       "      <th>what</th>\n",
       "      <th>work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.389888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.389888</td>\n",
       "      <td>0.389888</td>\n",
       "      <td>0.389888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296520</td>\n",
       "      <td>0.389888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.389888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.452123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687703</td>\n",
       "      <td>0.343851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.452123</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   advertised     after    broken    couple      days      does        is  \\\n",
       "0    0.000000  0.000000  0.622766  0.000000  0.000000  0.000000  0.473630   \n",
       "1    0.000000  0.389888  0.000000  0.389888  0.389888  0.389888  0.000000   \n",
       "2    0.452123  0.000000  0.000000  0.000000  0.000000  0.000000  0.687703   \n",
       "\n",
       "        not        of      this      what      work  \n",
       "0  0.000000  0.000000  0.622766  0.000000  0.000000  \n",
       "1  0.296520  0.389888  0.000000  0.000000  0.389888  \n",
       "2  0.343851  0.000000  0.000000  0.452123  0.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(sample)\n",
    "pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derived feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(1, 9)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6],\n",
       "       [7],\n",
       "       [8]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x[:, np.newaxis]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mPolynomialFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minteraction_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minclude_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Generate polynomial and interaction features.\n",
       "\n",
       "Generate a new feature matrix consisting of all polynomial combinations\n",
       "of the features with degree less than or equal to the specified degree.\n",
       "For example, if an input sample is two dimensional and of the form\n",
       "[a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "degree : integer\n",
       "    The degree of the polynomial features. Default = 2.\n",
       "\n",
       "interaction_only : boolean, default = False\n",
       "    If true, only interaction features are produced: features that are\n",
       "    products of at most ``degree`` *distinct* input features (so not\n",
       "    ``x[1] ** 2``, ``x[0] * x[2] ** 3``, etc.).\n",
       "\n",
       "include_bias : boolean\n",
       "    If True (default), then include a bias column, the feature in which\n",
       "    all polynomial powers are zero (i.e. a column of ones - acts as an\n",
       "    intercept term in a linear model).\n",
       "\n",
       "order : str in {'C', 'F'}, default 'C'\n",
       "    Order of output array in the dense case. 'F' order is faster to\n",
       "    compute, but may slow down subsequent estimators.\n",
       "\n",
       "    .. versionadded:: 0.21\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.preprocessing import PolynomialFeatures\n",
       ">>> X = np.arange(6).reshape(3, 2)\n",
       ">>> X\n",
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5]])\n",
       ">>> poly = PolynomialFeatures(2)\n",
       ">>> poly.fit_transform(X)\n",
       "array([[ 1.,  0.,  1.,  0.,  0.,  1.],\n",
       "       [ 1.,  2.,  3.,  4.,  6.,  9.],\n",
       "       [ 1.,  4.,  5., 16., 20., 25.]])\n",
       ">>> poly = PolynomialFeatures(interaction_only=True)\n",
       ">>> poly.fit_transform(X)\n",
       "array([[ 1.,  0.,  1.,  0.],\n",
       "       [ 1.,  2.,  3.,  6.],\n",
       "       [ 1.,  4.,  5., 20.]])\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "powers_ : array, shape (n_output_features, n_input_features)\n",
       "    powers_[i, j] is the exponent of the jth input in the ith output.\n",
       "\n",
       "n_input_features_ : int\n",
       "    The total number of input features.\n",
       "\n",
       "n_output_features_ : int\n",
       "    The total number of polynomial output features. The number of output\n",
       "    features is computed by iterating over all suitably sized combinations\n",
       "    of input features.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Be aware that the number of features in the output array scales\n",
       "polynomially in the number of features of the input array, and\n",
       "exponentially in the degree. High degrees can cause overfitting.\n",
       "\n",
       "See :ref:`examples/linear_model/plot_polynomial_interpolation.py\n",
       "<sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py>`\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/pyml/lib/python3.7/site-packages/sklearn/preprocessing/data.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PolynomialFeatures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.],\n",
       "       [ 2.,  4.],\n",
       "       [ 3.,  9.],\n",
       "       [ 4., 16.],\n",
       "       [ 5., 25.],\n",
       "       [ 6., 36.],\n",
       "       [ 7., 49.],\n",
       "       [ 8., 64.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X2 = poly.fit_transform(X)\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.],\n",
       "       [ 1.,  2.,  4.],\n",
       "       [ 1.,  3.,  9.],\n",
       "       [ 1.,  4., 16.],\n",
       "       [ 1.,  5., 25.],\n",
       "       [ 1.,  6., 36.],\n",
       "       [ 1.,  7., 49.],\n",
       "       [ 1.,  8., 64.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree=2, include_bias=True)\n",
    "X2_ = poly.fit_transform(X)\n",
    "X2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,   1.,   1.],\n",
       "       [  2.,   4.,   8.],\n",
       "       [  3.,   9.,  27.],\n",
       "       [  4.,  16.,  64.],\n",
       "       [  5.,  25., 125.],\n",
       "       [  6.,  36., 216.],\n",
       "       [  7.,  49., 343.],\n",
       "       [  8.,  64., 512.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X3 = poly.fit_transform(X)\n",
    "X3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan,  0.,  3.],\n",
       "       [ 4.,  5.,  9.],\n",
       "       [ 8.,  7.,  2.],\n",
       "       [ 4., nan,  6.],\n",
       "       [ 8.,  8.,  1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan = np.nan\n",
    "XX = np.array([\n",
    "    [nan, 0,   3],\n",
    "    [4,   5,   9],\n",
    "    [8,   7,   2],\n",
    "    [4,   nan, 6],\n",
    "    [8,   8,   1]\n",
    "])\n",
    "XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = np.array([12, 18, -3, 3, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Imputation transformer for completing missing values.\n",
       "\n",
       "Read more in the :ref:`User Guide <imputation>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "missing_values : integer or \"NaN\", optional (default=\"NaN\")\n",
       "    The placeholder for the missing values. All occurrences of\n",
       "    `missing_values` will be imputed. For missing values encoded as np.nan,\n",
       "    use the string value \"NaN\".\n",
       "\n",
       "strategy : string, optional (default=\"mean\")\n",
       "    The imputation strategy.\n",
       "\n",
       "    - If \"mean\", then replace missing values using the mean along\n",
       "      the axis.\n",
       "    - If \"median\", then replace missing values using the median along\n",
       "      the axis.\n",
       "    - If \"most_frequent\", then replace missing using the most frequent\n",
       "      value along the axis.\n",
       "\n",
       "axis : integer, optional (default=0)\n",
       "    The axis along which to impute.\n",
       "\n",
       "    - If `axis=0`, then impute along columns.\n",
       "    - If `axis=1`, then impute along rows.\n",
       "\n",
       "verbose : integer, optional (default=0)\n",
       "    Controls the verbosity of the imputer.\n",
       "\n",
       "copy : boolean, optional (default=True)\n",
       "    If True, a copy of X will be created. If False, imputation will\n",
       "    be done in-place whenever possible. Note that, in the following cases,\n",
       "    a new copy will always be made, even if `copy=False`:\n",
       "\n",
       "    - If X is not an array of floating values;\n",
       "    - If X is sparse and `missing_values=0`;\n",
       "    - If `axis=0` and X is encoded as a CSR matrix;\n",
       "    - If `axis=1` and X is encoded as a CSC matrix.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "statistics_ : array of shape (n_features,)\n",
       "    The imputation fill value for each feature if axis == 0.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "- When ``axis=0``, columns which only contained missing values at `fit`\n",
       "  are discarded upon `transform`.\n",
       "- When ``axis=1``, an exception is raised if there are rows for which it is\n",
       "  not possible to fill in the missing values (e.g., because they only\n",
       "  contain missing values).\n",
       "\u001b[0;31mInit docstring:\u001b[0m DEPRECATED: Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/pyml/lib/python3.7/site-packages/sklearn/preprocessing/imputation.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Imputer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6., 0., 3.],\n",
       "       [4., 5., 9.],\n",
       "       [8., 7., 2.],\n",
       "       [4., 5., 6.],\n",
       "       [8., 8., 1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Imputer(strategy=\"mean\").fit_transform(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6., 0., 3.],\n",
       "       [4., 5., 9.],\n",
       "       [8., 7., 2.],\n",
       "       [4., 6., 6.],\n",
       "       [8., 8., 1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Imputer(strategy=\"median\").fit_transform(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 0., 3.],\n",
       "       [4., 5., 9.],\n",
       "       [8., 7., 2.],\n",
       "       [4., 0., 6.],\n",
       "       [8., 8., 1.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Imputer(strategy=\"most_frequent\").fit_transform(XX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    Imputer(strategy=\"mean\"),\n",
    "    PolynomialFeatures(degree=2),\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('imputer',\n",
       "                 Imputer(axis=0, copy=True, missing_values='NaN',\n",
       "                         strategy='mean', verbose=0)),\n",
       "                ('polynomialfeatures',\n",
       "                 PolynomialFeatures(degree=2, include_bias=True,\n",
       "                                    interaction_only=False, order='C')),\n",
       "                ('linearregression',\n",
       "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "                                  normalize=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(XX, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12., 18., -3.,  3., -1.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
